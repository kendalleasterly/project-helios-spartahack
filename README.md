# Project Helios 

> **An AI-powered assistive navigation system for blind users using computer vision, real-time object detection, and conversational AI.**

Project Helios (branded as "a-eye") is a comprehensive mobile application that provides real-time spatial awareness and navigation assistance for blind users through advanced computer vision, YOLO11 object detection, and Google's Gemini AI.

## ğŸŒŸ Overview

Project Helios combines:
- **Real-time Visual Processing**: YOLO11-based object detection with spatial awareness
- **Conversational AI**: Natural language interaction via Google Gemini
- **Proactive Guidance**: Intelligent heuristics-driven navigation assistance
- **Wake Word Activation**: Hands-free "Helios" wake word detection
- **Mobile-First Design**: Native iOS app built with React Native and Expo

### Key Features

- ğŸ¯ **Real-time Object Detection**: YOLO11 Nano for optimal accuracy/speed balance
- ğŸ“ **Spatial Awareness**: Semantic positioning (Left/Center/Right) and distance estimation
- ğŸš¨ **Emergency Detection**: Automatic hazard detection with haptic feedback
- ğŸ—£ï¸ **Voice Interaction**: Continuous speech recognition with wake word detection
- ğŸ¤– **AI Assistant**: Contextual, conversational responses powered by Gemini
- ğŸ“± **Mobile App**: Cross-platform support for iOS

## ğŸ—ï¸ Architecture

The system consists of three main components:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                      MOBILE APP (iOS)                        â”‚
â”‚  â€¢ Camera capture (React Native Vision Camera)              â”‚
â”‚  â€¢ Continuous speech recognition                            â”‚
â”‚  â€¢ Socket.IO client for real-time communication             â”‚
â”‚  â€¢ Text-to-speech output                                    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â”‚ WebSocket (Socket.IO)
                         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚               BACKEND SERVER (Python/FastAPI)                â”‚
â”‚  â€¢ YOLO11 object detection                                  â”‚
â”‚  â€¢ Heuristics engine (when to speak)                        â”‚
â”‚  â€¢ Wake word detection                                      â”‚
â”‚  â€¢ Scene history tracking                                   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â”‚ API Calls
                         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    GOOGLE GEMINI AI                          â”‚
â”‚  â€¢ Vision mode: Proactive navigation guidance               â”‚
â”‚  â€¢ Conversation mode: Answer user questions                 â”‚
â”‚  â€¢ Context-aware responses                                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Data Flow

1. **Visual Input**: iPhone camera captures frames at ~1 FPS
2. **Detection**: YOLO11 processes frames and identifies objects with spatial data
3. **Decision**: Heuristics engine determines if guidance is needed
4. **Intelligence**: Gemini AI generates contextual, conversational responses
5. **Output**: Text-to-speech provides audio guidance to the user

### Dual-Pipeline Architecture

**Vision Pipeline** (Proactive):
- Monitors environment continuously
- Speaks when obstacles or hazards detected
- Uses heuristics to avoid over-speaking

**Conversation Pipeline** (Reactive):
- Activated by "Helios" wake word
- Answers user questions
- Provides detailed environmental descriptions

## ğŸš€ Getting Started

### Prerequisites

- **Backend**: Python 3.10+, pip
- **Mobile**: Node.js 18+, npm, Expo CLI
- **iOS Development**: macOS with Xcode (for iOS builds)

### Quick Setup

1. **Clone the repository**:
   ```bash
   git clone https://github.com/kendalleasterly/project-helios-spartahack.git
   cd project-helios-spartahack
   ```

2. **Set up the backend**:
   ```bash
   cd backend
   python -m venv venv
   source venv/bin/activate  # On Windows: venv\Scripts\activate
   pip install -r requirements.txt
   python server.py
   ```
   
   See [backend/README.md](backend/README.md) for detailed setup instructions.

3. **Set up the mobile app**:
   ```bash
   cd mobile
   npm install
   cp .env.example .env
   # Edit .env and update BACKEND_SERVER_URL with your server IP
   npm start
   ```
   
   See [mobile/README.md](mobile/README.md) for detailed setup instructions.

### API Configuration

You'll need to configure:
- **Google Gemini API Key**: For AI-powered responses
- **Deepgram API Key** (optional): For enhanced speech recognition
- **Backend Server URL**: In mobile app's `.env` file

## ğŸ“š Documentation

Each component has detailed documentation:

- **[Backend Documentation](backend/README.md)**: YOLO11 setup, API endpoints, spatial logic
- **[Mobile Documentation](mobile/README.md)**: React Native setup, module building, commands
- **[Gemini Architecture](backend/GEMINI.md)**: AI decision flow, prompts, personality design
- **[Wake Word API](backend/WAKE_WORD_API.md)**: Voice interaction implementation

## ğŸ¨ Project Structure

```
project-helios-spartahack/
â”œâ”€â”€ backend/                    # Python FastAPI server
â”‚   â”œâ”€â”€ server.py              # Main server with Socket.IO
â”‚   â”œâ”€â”€ heuristics.py          # Decision engine for when to speak
â”‚   â”œâ”€â”€ gemini_service.py      # Gemini API integration
â”‚   â”œâ”€â”€ contextual_gemini_service.py  # Context-aware Gemini calls
â”‚   â””â”€â”€ requirements.txt       # Python dependencies
â”‚
â”œâ”€â”€ mobile/                    # React Native mobile app
â”‚   â”œâ”€â”€ app/                   # Expo Router pages
â”‚   â”œâ”€â”€ components/            # React components
â”‚   â”œâ”€â”€ expo-stream-audio/     # Custom audio streaming module
â”‚   â”œâ”€â”€ hooks/                 # React hooks
â”‚   â””â”€â”€ package.json           # Node.js dependencies
â”‚
â””â”€â”€ README.md                  # This file
```

## ğŸ”¬ Technology Stack

### Backend
- **FastAPI**: Modern Python web framework
- **Socket.IO**: Real-time bidirectional communication
- **YOLO11**: State-of-the-art object detection
- **OpenCV**: Image processing
- **Google Gemini 2.5 Flash**: Conversational AI

### Mobile
- **React Native**: Cross-platform mobile framework
- **Expo**: Development and build tooling
- **React Native Vision Camera**: Camera access
- **@react-native-voice/voice**: Speech recognition
- **Socket.IO Client**: Real-time server communication

## ğŸ¯ Key Innovations

### Heuristics-Driven Guidance
Instead of having the AI decide when to speak, we use deterministic heuristics based on YOLO detection data:
- **Emergency**: Immediate response for vehicles/hazards
- **Alert**: Objects within 3 feet
- **Guidance**: Obstacles in walking path
- **Info**: New important objects detected

This approach provides:
- âœ… Faster response times (no AI decision latency)
- âœ… More reliable behavior (deterministic logic)
- âœ… Reduced API costs (fewer unnecessary calls)
- âœ… Better user experience (predictable assistance)

### Helios Personality
The AI assistant is designed with a distinct personality:
- **Warm but not patronizing**: Helpful friend, not a robot
- **Direct but not robotic**: Gets to the point naturally
- **Calm in emergencies**: Steady guidance under pressure
- **Honest about uncertainty**: Admits when it can't see clearly

Example interactions:
- Emergency: "Stop! Car left!"
- Guidance: "Chair ahead, veer right."
- Conversation: "Yeah, there's a door about 10 feet ahead."

## ğŸ§ª Development

### Running Tests
```bash
# Backend tests (if available)
cd backend
python -m pytest

# Mobile type checking
cd mobile
npm run typecheck
```

### Building the App
```bash
# iOS development build
cd mobile
npm run build:dev

# Clean cache if needed
npm run metro:clean
```

### Local Module Development
The mobile app includes a custom `expo-stream-audio` module:
```bash
# After JS changes in expo-stream-audio/src
npm run module:build

# After native changes in expo-stream-audio/ios
npm run build:dev
```

## ğŸ¤ Contributing

This project was developed for SpartaHack 9. Contributions are welcome!

1. Fork the repository
2. Create a feature branch: `git checkout -b feature-name`
3. Commit your changes: `git commit -m 'Add feature'`
4. Push to the branch: `git push origin feature-name`
5. Submit a pull request

## ğŸ“„ License

MIT License - See individual component documentation for details.

## ğŸ™ Acknowledgments

- **SpartaHack 9**: For providing the opportunity to build this project
- **Ultralytics**: For the YOLO11 object detection model
- **Google**: For Gemini AI API access
- **Expo Team**: For excellent React Native tooling

## ğŸ“ Support

For questions or issues:
- Check component-specific README files
- Review backend logs with DEBUG level enabled
- Open an issue on GitHub

---

**Built with â¤ï¸ for accessibility and inclusion**
